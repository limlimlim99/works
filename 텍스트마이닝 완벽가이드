{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f033a179",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2882725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk 라이브러리 다운로드\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdc5305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     --------                                 0.3/1.5 MB 9.6 MB/s eta 0:00:01\n",
      "     -----------------------                  0.9/1.5 MB 11.4 MB/s eta 0:00:01\n",
      "     -------------------------------------    1.4/1.5 MB 11.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "                                              0.0/298.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 298.0/298.0 kB 9.3 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.5.5-cp310-cp310-win_amd64.whl (267 kB)\n",
      "                                              0.0/267.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 267.9/267.9 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "                                              0.0/77.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.8.1 regex-2023.5.5 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303a02f",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cd2b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text minig class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text minig class!\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80892ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
     ]
    }
   ],
   "source": [
    "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\"\n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "print(tokenizer.tokenize(paragraph_french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51411b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
     ]
    }
   ],
   "source": [
    "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\"\n",
    "print(sent_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730ded1",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87a5227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'minig', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09cc2231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'minig', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c1e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6d6b1",
   "metadata": {},
   "source": [
    "### 정규표현식을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "639f7370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(\"[abc]\", \"How are you, boy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621651d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '5', '9']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[0123456789]\", \"3a7b5c9d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40a17bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]\", \"3a 7b_ ' .^&5c9d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88c1929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_', '__', '___']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[_]+\", \"a_b, c__d, e___f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00a4c750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'are', 'you', 'boy']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]+\", \"How are you, boy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e57ab1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'oooo', 'oooo', 'ooo']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[o]{2,4}\", \"oh, hoow are yoooou, boooooooy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "581c6d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'I', \"can't\", 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# regular expression(정규식)을 이용한 tokenizer\n",
    "# 단어 단위로 tokenize \\w: 문자나 숫자를 의미. 즉 문자나 숫자 혹은 '가 반복되는 것을 찾아냄\n",
    "tokenizer =  RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "# can't를 하나의 단어로 인식\n",
    "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d796982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"can't\", 'there']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Sorry, I can't go there.\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")\n",
    "print(tokenizer.tokenize(text1.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0798c",
   "metadata": {},
   "source": [
    "### 노이즈와 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fb3b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들\n",
    "english_stops = set(stopwords.words('english')) # 반복이 되지 않도록 set으로 변환\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower()) # word_tokenize로 토큰화\n",
    "\n",
    "# stopwords를 제외한 단어들만으로 list를 생성\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "\n",
    "print((result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb571cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'their', 'again', 'more', 'what', 'than', 'which', \"isn't\", 'yourself', 'below', 'before', 'ourselves', 'yourselves', 'themselves', 'your', 'mightn', 'any', \"wasn't\", 'me', \"hasn't\", 'is', 'my', \"you'd\", 'when', 'wasn', 'it', 'how', \"mightn't\", 'few', 'such', 'whom', 'some', 'was', 've', 'hadn', 'both', 'but', 's', 'up', 'out', 're', 'no', 'shouldn', 'down', 'be', 'or', 'been', 'an', 'with', \"won't\", 'don', 'herself', 'off', 'yours', 'over', 'too', 'hers', 'just', 'she', 'until', 'him', 'here', \"she's\", \"that'll\", \"you're\", 'm', 'this', 'doesn', 'most', 'other', 'shan', 'isn', \"didn't\", 'haven', \"should've\", \"needn't\", 'theirs', 'had', 'them', 'into', \"you've\", 'himself', 'above', 'while', 'about', 'am', 'that', 'we', \"haven't\", 'why', 'only', \"hadn't\", 'under', 'have', 'wouldn', 'ma', 'to', 'itself', 'of', 'does', 'mustn', \"it's\", 'between', 'then', 'there', 'now', 'after', 'doing', 'aren', 'you', 'each', 'by', \"aren't\", \"mustn't\", 'won', 'same', 'and', 'the', 'can', 'as', 'couldn', 'very', 'from', 'if', \"shouldn't\", 'd', 'a', \"doesn't\", 'are', 'needn', 'at', 'these', 'myself', 'having', 'nor', 'hasn', 'against', 'on', \"shan't\", 'who', 'should', 'he', 'did', 'didn', \"don't\", \"weren't\", 'weren', 'for', 'where', 'll', 'in', 'do', \"you'll\", 'ours', 'our', 'her', 'once', 'because', 'own', 'so', 'will', \"couldn't\", 'those', 'being', 'o', 'they', 'through', 'further', 'his', 'its', 'all', \"wouldn't\", 't', 'i', 'not', 'y', 'during', 'were', 'ain', 'has'}\n"
     ]
    }
   ],
   "source": [
    "# nltk가 제공하는 영어 stopword를 확인\n",
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "692535ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "my_stopword = ['i', 'go', 'to']\n",
    "result = [word for word in tokens if word not in my_stopword]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e6e5a",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed8c7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39db30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화와 결합해 어간 추출\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para) #토큰화 실행\n",
    "print(tokens)\n",
    "result = [stemmer.stem(token) for token in tokens] #모든 토큰에 대해 스테밍 실행\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7602ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "# 랭카스터 스테머\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54f499",
   "metadata": {},
   "source": [
    "### 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e117c020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookery\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v')) # 품사를 지정\n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cc26b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming result: believ\n",
      "stemming result: belief\n",
      "stemming result: believe\n"
     ]
    }
   ],
   "source": [
    "# lemmatizing과 stemming 비교\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print('stemming result:', stemmer.stem('believes'))\n",
    "print('stemming result:', lemmatizer.lemmatize('believes'))\n",
    "print('stemming result:', lemmatizer.lemmatize('believes', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef8a3b",
   "metadata": {},
   "source": [
    "# 품사 태깅\n",
    "\n",
    "- 명사 : 이름을 나타내는 낱말\n",
    "- 대명사 : 이름을 대신 가리키는 낱말\n",
    "- 수사 : 수량이나 순서를 가리키는 낱말\n",
    "- 조사 : 도와주는 낱말\n",
    "- 동사 : 움직임을 나타내는 낱말\n",
    "- 형용사 : 상태나 성질을 나타내는 낱말\n",
    "- 관형사 : 체언을 꾸며주는 낱말\n",
    "- 부사 : 주로 용언을 꾸며주는 낱말\n",
    "- 감탄사 : 놀람, 느낌, 부름, 대답을 나타내는 낱말"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "221398dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'JJ'), ('min-ing', 'JJ'), ('class', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "### NLTK를 활용한 품사 태깅\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text min-ing class!\")\n",
    "print(nltk.pos_tag(tokens))\n",
    "\n",
    "# nltk는 펜 트리뱅크 태그 집합을 사용하므로 위 표를 보면 품사 약어의 의미를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e989edc",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtagsets\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mhelp/tagsets/upenn_tagset.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USER/nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhelp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupenn_tagset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\help.py:27\u001b[0m, in \u001b[0;36mupenn_tagset\u001b[1;34m(tagpattern)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupenn_tagset\u001b[39m(tagpattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 27\u001b[0m     \u001b[43m_format_tagset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupenn_tagset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagpattern\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\help.py:46\u001b[0m, in \u001b[0;36m_format_tagset\u001b[1;34m(tagset, tagpattern)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_tagset\u001b[39m(tagset, tagpattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 46\u001b[0m     tagdict \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhelp/tagsets/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tagpattern:\n\u001b[0;32m     48\u001b[0m         _print_entries(\u001b[38;5;28msorted\u001b[39m(tagdict), tagdict)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtagsets\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mhelp/tagsets/upenn_tagset.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USER/nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35d27273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'good', 'see', 'Let', 'start', 'text', 'min-ing', 'class']\n"
     ]
    }
   ],
   "source": [
    "# 원하는 품사의 단어들만 추출\n",
    "my_tag_set = ['NN', 'VB', 'JJ']\n",
    "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "print(my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33438644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello/NNP', 'everyone/NN', './.', 'It/PRP', \"'s/VBZ\", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', \"'s/POS\", 'start/VB', 'our/PRP$', 'text/JJ', 'min-ing/JJ', 'class/NN', '!/.']\n"
     ]
    }
   ],
   "source": [
    "# 단어에 품사 정보를 추가해 구분\n",
    "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]\n",
    "print(words_with_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "899218dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망이', '적다면', '그', '누가', '세상을', '비추어줄까', '.', '정희선', ',', '희망', '공부']\n",
      "[('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비추어줄까', 'NNP'), ('.', '.'), ('정희선', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# 한들 형태소 분석과 품사 태깅\n",
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망이 적다면\n",
    "그 누가 세상을 비추어줄까.\n",
    "정희선, 희망 공부'''\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371b590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
